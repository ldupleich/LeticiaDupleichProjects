{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcf715c3-7bc0-4cf5-beb3-302c485b8262",
   "metadata": {},
   "source": [
    "# Hyperparameter Exploration\n",
    "## This notebook is a research notebook on what woudl be the best features for our LightGBM model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce07ae00-1979-4874-a163-0e020d1a842a",
   "metadata": {},
   "source": [
    "Conducting research we found that the most significat parameters to chieve optimal performance are n_estimators, learning_rate, num_leaves, max_depth, and min_child_samples. While other parameters can also be optimized depending upon feature_fraction, bagging_fraction, bagging_freq, and regularization parameters (L1/L2) for preventing overfitting. \n",
    "\n",
    "Since our dataset is not very large, we wouldnt be needing feature_fraction, bagging_fraction, bagging_freq and lambda_l1, lambda_l2. These are good for regularization. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4d4730-edd3-422e-b9fc-0dd747178b98",
   "metadata": {},
   "source": [
    "## Core Parameters:\n",
    "\n",
    "- n_estimators (or num_boost_round) sets the boosting iterations count. Start with a moderate value (e.g., 100-500) and use early stopping to find the optimal point.\n",
    "\n",
    "- learning_rate tunes the step size per pass. Smaller learning rates lead to improved generalization but require more boosting iterations. We could start with 0.1 and reduce to 0.01 or even further to 0.001.\n",
    "\n",
    "- num_leaves limits the number of leaves in each tree. A larger num_leaves allows for more complex trees but can lead to overfitting. A common guideline found is to set it around 2^(max_depth). \n",
    "\n",
    "- max_depth limits the depth of the trees. A smaller max_depth is reccomended as to prevent overfitting. We should experiment with values between 3 and 10. \n",
    "\n",
    "- min_child_samples (or min_data_in_leaf) sets the minimum number of data points that should live in a leaf. This averts overfitting by not allowing leaves to be generated from just a few data points.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9332a797-38ab-4c12-b550-52d6fa4f040e",
   "metadata": {},
   "source": [
    "## Other Parameters:\n",
    "\n",
    "There are other important parameters that we could experiment with. All of these prevent or reduce overfitting or complexity + high dimensionality somehow. Some of these being:\n",
    "\n",
    "- feature_fraction controls the ratio of features to be selected randomly for each tree. This can help with high dimensionality and overfitting.\n",
    "\n",
    "- bagging_fraction and bagging_freq permit random data sampling. bagging_freq specifies bagging frequency.\n",
    "\n",
    "- lambda_l1 and lambda_l2: L1 and L2 regularization coefficients penalize model complexity.\n",
    "\n",
    "- min_gain_to_split specifies the minimum gain required for a split of a node. Used to regulate tree complexity.\n",
    "\n",
    "- num_threads specifies how many threads LightGBM will be using. We should use the most available threads for faster training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1201b491-73ec-41fa-aa76-2b780256de9b",
   "metadata": {},
   "source": [
    "Some other important aspects for working with LightGBM I have found are: \n",
    "\n",
    "- Using the base parameters and gradually adjust them based on our data and goal.\n",
    "\n",
    "- Use early stopping because this is crucial in finding the optimal number of boosting steps and preventing overfitting.\n",
    "\n",
    "- Cross-validation: estimate your model with cross-validation to get a reliable measure of its performance.\n",
    "\n",
    "- Maybe use the GPU-accelerated version of LightGBM for faster training.\n",
    "\n",
    "- We could try tuning a few major parameters at a time and changing them gradually as oposed to tuning the parameters all  simultaneously. \n",
    "\n",
    "- For regularization we could use parameters like feature_fraction, bagging_fraction, and regularization parameters (L1/L2) to prevent overfitting, especially for large datasets.\n",
    "\n",
    "- Because LightGBM uses a leaf-wise tree growing approach, producing deeper trees, we can adjust num_leaves and max_depth for fine-tuning the trees' complexity. -> these two last points are using the parameters I mentioned above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b632ab3-f4fb-4ea2-9ba6-32d56d6aea39",
   "metadata": {},
   "source": [
    "Before we can start, we need to have our LightGBM model and test it manually with default parameters. This helps us verify the data pipeline, check for data issues, and get a performance baseline. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e0670a-1da9-4aab-8d62-458548a369fd",
   "metadata": {},
   "source": [
    "## Hyperparamters Tuning with Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd26bba8-4bc5-4895-b28c-5cfdc4625e64",
   "metadata": {},
   "source": [
    "A grid-search is an automated way of trying different combinations of hyperparameters to find the best one for our model! -> This will be useful when we start testing!\n",
    "\n",
    "An example is the following that I got from chatGPT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0c2e0dd4-0bda-446c-8bb7-4abe4f8658e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GridSearchCV' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 10\u001b[0m\n\u001b[0;32m      1\u001b[0m param_grid \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_leaves\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m31\u001b[39m, \u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m100\u001b[39m],\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m20\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolsample_bytree\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m0.6\u001b[39m, \u001b[38;5;241m0.8\u001b[39m, \u001b[38;5;241m1.0\u001b[39m],\n\u001b[0;32m      8\u001b[0m }\n\u001b[1;32m---> 10\u001b[0m grid \u001b[38;5;241m=\u001b[39m GridSearchCV(LGBMRegressor(), param_grid, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneg_root_mean_squared_error\u001b[39m\u001b[38;5;124m'\u001b[39m, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     11\u001b[0m grid\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest parameters:\u001b[39m\u001b[38;5;124m\"\u001b[39m, grid\u001b[38;5;241m.\u001b[39mbest_params_)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'GridSearchCV' is not defined"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'num_leaves': [31, 50, 100],\n",
    "    'max_depth': [-1, 10, 20],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'n_estimators': [100, 200],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(LGBMRegressor(), param_grid, scoring='neg_root_mean_squared_error', cv=5, verbose=1)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters:\", grid.best_params_)\n",
    "print(\"Best RMSE:\", -grid.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48773e6b-762c-492a-b799-9ac8c01de735",
   "metadata": {},
   "source": [
    "Before we can do this we have to make sure the data is pre-processesd (we did) but also transform text-based features like actors, directors, genre, theme by encoding into numbers (one-hot, label encoding, frequency encoding). And of course split the data into training and testing. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45257d8-a37c-4ebb-95dd-15530acd5d25",
   "metadata": {},
   "source": [
    "We could also use RandomizedSearchCV, which is a method from scikit-learn that would also help us find the best hyperparameters for our model, but its faster and more efficient when we have more parameters or large ranges to explore. This is why it would be a good initial use. \n",
    "\n",
    "In contrast to GridSearchCV, RandomizedSearchCv tries a random subset of the combinations of parameters we give it (as opposed to all). It then finds the near-best with fewer trials. GridSearch however, is guaranteed to find the best in the grid, not just a near-best; that's why it would be a good option to use both, starting with RandomizedSearchCV. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c59b4ad-e49e-48b7-af82-a1f734b46303",
   "metadata": {},
   "source": [
    "## RandomizedSearchCV with LightGBM example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbff8b9-6713-4954-8150-e47a753ed1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from lightgbm import LGBMRegressor\n",
    "from scipy.stats import randint, uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fca91ac-7cf6-4808-9076-af3211577817",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = {\n",
    "    'num_leaves': randint(20, 150),\n",
    "    'max_depth': randint(3, 20),\n",
    "    'learning_rate': uniform(0.01, 0.2),\n",
    "    'n_estimators': randint(50, 500),\n",
    "    'subsample': uniform(0.5, 0.5),\n",
    "    'colsample_bytree': uniform(0.5, 0.5),\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    LGBMRegressor(),\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=50,  # number of combinations they\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    cv=5,\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters:\", search.best_params_)\n",
    "print(\"Best RMSE:\", -search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd34af30-0c7e-416e-a062-26085068d163",
   "metadata": {},
   "source": [
    "### Another Example of how it would work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "072c7e8e-d38a-481d-9c2b-296d961061da",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LGBMRegressor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 14\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m randint, uniform\n\u001b[0;32m      4\u001b[0m param_dist \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_leaves\u001b[39m\u001b[38;5;124m'\u001b[39m: randint(\u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m100\u001b[39m),\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m'\u001b[39m: randint(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m15\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolsample_bytree\u001b[39m\u001b[38;5;124m'\u001b[39m: uniform(\u001b[38;5;241m0.6\u001b[39m, \u001b[38;5;241m0.4\u001b[39m),\n\u001b[0;32m     11\u001b[0m }\n\u001b[0;32m     13\u001b[0m search \u001b[38;5;241m=\u001b[39m RandomizedSearchCV(\n\u001b[1;32m---> 14\u001b[0m     LGBMRegressor(),\n\u001b[0;32m     15\u001b[0m     param_distributions\u001b[38;5;241m=\u001b[39mparam_dist,\n\u001b[0;32m     16\u001b[0m     n_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m,  \u001b[38;5;66;03m# try 30 combinations\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneg_root_mean_squared_error\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     18\u001b[0m     cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[0;32m     19\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m     20\u001b[0m     random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[0;32m     21\u001b[0m )\n\u001b[0;32m     23\u001b[0m search\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest RMSE:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m-\u001b[39msearch\u001b[38;5;241m.\u001b[39mbest_score_)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'LGBMRegressor' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "param_dist = {\n",
    "    'num_leaves': randint(20, 100),\n",
    "    'max_depth': randint(3, 15),\n",
    "    'learning_rate': uniform(0.01, 0.1),\n",
    "    'n_estimators': randint(50, 300),\n",
    "    'subsample': uniform(0.6, 0.4),\n",
    "    'colsample_bytree': uniform(0.6, 0.4),\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    LGBMRegressor(),\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=30,  # try 30 combinations\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    cv=3,\n",
    "    verbose=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best RMSE:\", -search.best_score_)\n",
    "print(\"Best Params:\", search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81be9a46-9653-44fd-864b-3ee3ebbbc547",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
